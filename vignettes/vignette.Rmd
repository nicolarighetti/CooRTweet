---
title: "Introduction to CooRTweet"
author: "Nicola Righetti & Paul Balluff"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to CooRTweet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(CooRTweet)

```


# Introduction

We introduce `CooRTweet` an R-Package for detecting coordinated behavior on Twitter. Coordinated behavior has been defined as "the act of making people and/or things involved in organized cooperation" [@Giglietto2020, p. 872]. 

Coordinated behavior on social media has been used for political astroturfing [@Keller2020], spreading inappropriate content [@Giglietto2020], and activism. Detecting such behavior is crucial for academic research and investigative journalism.

Software for academic research and investigative journalism has been developed in the last few years to detect coordinated behavior, such as the CooRnet R package [@Giglietto2020], which detects Coordinated Link Sharing Behavior (CLSB) and Coordinated Image Sharing on Facebook and Instagram ([CooRnet website](http://coornet.org/)).

The `CooRTweet` package builds on the existing literature on coordinated behavior and the experience of previous software to provide an easy-to-use tool for detecting various coordinated networks on Twitter. The package is powered by `data.table` [@datatable] which makes efficient use of memory and is considerably fast.

The package works with data retrieved from the Twitter Academic API V2 in JSON format. The data can be conveniently collected with the R package `academictwitteR` and the function `get_all_tweets`, which retrieves tweets and users' information at once. The `CooRTweet` convenience function `load_data` implements the state-of-the-art approach to parse large amount of JSON data in the fastest way currently possible [@RcppSimdJson].

# Modelling Coordinated behavior: Key Parameters

Following the standard operationalization in literature, two or more users are defined as coordinated when they perform the *same action* ($a_i \in A$) at least $r$ times, within a predefined time interval $\Delta t$. 

This so-called "same action" ($a_i$) can be operationalized in a variety of ways:

- sharing the same URL
- using the same hashtag 
- retweeting the same original tweet
- theoretically, sharing any kind of uniquely identifiable content that can be shared on social media

In `CooRTweet` we refer to the content on which we track the "same actions" as *objects*. $n(A) = N$ is the total number of potentially coordinated actions or *objects*. For example, if your dataset has 100 unique URLs then one URL is a object $a_i$ and $n(A) = N = 100$. 

The `detect_coordinated_groups()` function permits to identify coordinated users through two main parameters: `min_repetition` ($r$) and `time_window` ($\Delta t$). Based on these two parameters the core function of `CooRTweet` identifies coordinated Twitter actors and networks. 

To identify coordinated networks, all pairs of users that performed the same action are computed, and the resulting list is then filtered according to the parameters `time_window` and `min_repetition`. Given a set of $A$ actions and their total count $N = n(A)$ the possible pairs of coordinated actions are given by $\frac{N!}{2!(N-2)!}$ (in R: `base::choose(N, k=2)`). The number of possible combinations increases with the number of actions $N$, requiring increasing computational power. The number of actions $N$ also generally increases when analyzing longer time intervals (`time_window`) and when using a low threshold for the number of repetitions (`min_repetitions`). CooRTweet makes this computation as efficient as possible using fast and memory-efficient `data.table` syntax.

There are no standard parameters for `min_repetition` and `time_window` as they depend on the theoretical assumptions when investigating coordinated behavior. For example, if the assumption is that most of the coordinated users are non-human actors (e.g., bots), then a very short `time_window` of less than 10 seconds seems appropriate ($\Delta t = 10$). In general, *the choice of the parameter depends on the particular case that you want to analyze, and it is recommended to test different parameter combinations*.


# A Usage Example

We provide an anonymized version of a real dataset of coordinated tweets by pro-government users in Russia [@Kulichkina2022]. You can load the sample dataset as follows:


```{r results='hide'}
library(CooRTweet)
russian_coord_tweets
```

The dataset has four columns which is the minimum required input data for detecting coordinated behavior:

- `object_id`: the coordinated content. In this example the ID of retweeted content.
- `id_user`: the unique ID of a twitter user.
- `content_id`: the unique IDs of the twitter users' posts. 
- `timestamp_share`: the exact time `content_id` was posted by the user.

The length of `content_id` should be the same as the number of rows of your input data

```{r results='hide'}
length(russian_coord_tweets$content_id) == nrow(russian_coord_tweets)

```

Let's assume that we want to detect coordinated behavior with a `min_repetition` of 5 within a `time_window` of 10 seconds. We can call the main function `detect_coordinated_groups()` as follows:

```{r results='hide'}
result <- detect_coordinated_groups(russian_coord_tweets, 
                                    min_repetition = 5, 
                                    time_window = 10)
```

The `result` is a `data.table` that only includes the users and their contents that were identified as coordinated with the given parameters. The `result` is in a wide-format, where it shows the time difference (`time_delta`) between two posts (`content_id` and `content_id_y`).

We can quickly get some summary statistics by using the provided convenience functions `group_stats()` and `user_stats()`. If we are interested in the content that users share in a coordinated fashion, we can call `group_stats()` and pass in our `results` table:

```{r results='hide'}
summary_groups <- group_stats(result)

```

`summary_groups` shows how many coordinated users (column `users`) participated for each unique shared object (`object_id`), how many coordinated posts (`posts`) were published and their average time delay (`mean_time_delta`). In this case, we have retweets, therefore, you can interpret the results as follows: 2 users retweeted the tweet with the id `4a5c13a7f533d6ebc27bec5581ad5a9f` within an average time delay of 2 seconds.

If you are interested in understanding more about the users you can call `user_stats()`:

```{r results='hide'}
summary_users <- user_stats(result)

```

`summary_users` shows that the user with the ID `5c10a1e23d788292981b40e4006f4563` shared a total of 9 coordinated posts and within an average time window of 4.2 seconds. 

# Using your own data

Of course, you want to use the package with your own data that you retrieved from the Twitter API (V2). We guide you here quickly through the process.

## Load Raw Data and Preprocess

We assume that all your tweets are stored as JSON files in a directory. You can load the JSON data with the `load_tweets_json()` and `load_twitter_users_json()` functions

```{r results='hide', eval=FALSE}
# load data

raw <- load_tweets_json('path/to/data/with/jsonfiles')
users <- load_twitter_users_json('path/to/data/with/jsonfiles')
```

**If you cannot load your Twitter data, please feel free to raise an issue in our [Github repository](https://github.com/nicolarighetti/CooRTweet/). We are happy to help!**

Twitter data is nested and difficult to handle, so we also provide a simple pre-processing function that unnests the data:

```{r results='hide', eval=FALSE}
# preprocess (unnest) data

tweets <- preprocess_tweets(raw)
users <- preprocess_twitter_users(users)
```

The resulting `tweets` is a named list, where each item is a `data.table`. The five `data.table`s are: `tweets`, `referenced`, `urls`, `mentions`, and `hashtags`. This keeps the data sorted and avoids redundant rows.

To access the tweets you can simply use `tweets$tweets` and view your dataset.

## Coordination Detection and Reshaping

The `reshape_tweets` function makes it possible to reshape Twitter data for detecting different types of coordinated behavior. The parameter `intent` of this function permits to choose between different options: `retweets`, for coordinated retweeting behavior; `hashtags`, for coordinated usage of hashtags; `urls` to detect coordinated link sharing behavior; `urls_domain` to detect coordinated link sharing behavior at the domain level.

### Coordination by Retweets

```{r results='hide', eval=FALSE}
# reshape data
retweets <- reshape_tweets(tweets, intent = "retweets")

# detect coordinated tweets
result <- detect_coordinated_groups(retweets, time_window = 60, min_repetition = 10)
```

### Coordination by Hashtags

```{r results='hide', eval=FALSE}
hashtags <- reshape_tweets(tweets, intent = "hashtags")
result <- detect_coordinated_groups(hashtags, time_window = 60, min_repetition = 10)
```

### Coordination by Link Sharing

```{r results='hide', eval=FALSE}
urls <- reshape_tweets(tweets, intent = "urls")
result <- detect_coordinated_groups(urls, time_window = 60, min_repetition = 10)
```

### Coordination by Link Sharing (considering only the domain)

```{r results='hide', eval=FALSE}
urls <- reshape_tweets(tweets, intent = "urls_domain")
result <- detect_coordinated_groups(urls, time_window = 60, min_repetition = 10)
```

### Get summaries of results

There are two functions that give summaries of the `result` data: `group_stats()` and `user_stats()`. 

To get insights on the coordinated content (groups), use `group_stats()`

```{r results='hide', eval=FALSE}
summary_groups <- group_stats(result)
```

It returns a `data.table` which shows the group statistics for total count of unique users, total posts in per group, and average time delta per group. If your group analysis is focused on retweets, you can join the data back as follows:

```{r results='hide', eval=FALSE}
library(data.table)
# rename tweet column
setnames(summary_groups, "object_id", "tweet_id")
summary_groups <- tweets$tweets[summary_groups, on = "tweet_id"]
```

If you are interested in the user statistics, you can pass `result` into `user_stats()`

```{r results='hide', eval=FALSE}
summary_users <- user_stats(result)
```

It provides summary statistics for each user that participated in coordinated behavior: total coordinated posts shared, and average time delta. High number of posts shared and low average time delta are indicators for highly coordinated (and potentially automated) user behavior.

You can rejoin these summary statistics with the original data as follows (using `data.table` syntax):

```{r results='hide', eval=FALSE}
library(data.table)

# rename user column
setnames(summary_users, "id_user", "user_id")

# join with pre-processed user data
summary_users <- users[summary_users, on = "user_id"]
```

### Generate a network

We provide a utility function to transform the `result` to an [`igraph`](https://r.igraph.org/) object for further analysis. In this example, we want to investigate the coordinated content, and how they are connected.

```{r results='hide', eval=FALSE}
library(igraph)

coord_graph <- generate_network(result, intent = "objects")

# E.g., get the degree of each node for filtering
igraph::V(coord_graph)$degree <- igraph::degree(coord_graph)

# Or we can run a community detection algorithm
igraph::V(coord_graph)$cluster <- igraph::cluster_louvain(coord_graph)$membership
```

Then, we can join the graph back to the original `data.table`, with additional information, such as the cluster for each content:

```{r results='hide', eval=FALSE}
library(data.table)
dt <- data.table(tweet_id=V(coord_graph)$name,
                cluster=V(coord_graph)$cluster,
                degree=V(coord_graph)$degree)

dt_joined <- tweets$tweets[dt, on = "tweet_id"]
```


# References

